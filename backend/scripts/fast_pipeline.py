import os, re, time, asyncio, aiohttp
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import requests

API_KEY = "acb95e5a2989c1fe3507d7119fb16cf35f331355485bf12f2683eb153ccc1f5e"
headers = {"User-Agent": "Mozilla/5.0"}
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

# ---------------- ê³µí†µ ìœ í‹¸ ----------------
def is_series_volume(title):
    title = title.lower()
    patterns = [
        r"\b\d+\s*ê¶Œ\b", r"ì œ\s*\d+\s*ê¶Œ", r"\bvol\.?\s*\d+",
        r"\bvolume\s*\d+", r"\bbook\s*\d+", r"\bpart\s*\d+",
        r"ì‹œì¦Œ\s*\d+", r"[\[\(]?\d+\s*ê¶Œ[\]\)]?", r"\s\d{1,2}$", r"[^\d]\d{1,2}$"
    ]
    return any(re.search(p, title) for p in patterns)

def clean_title_prefix(title):
    title = re.sub(r"[\s\(\[].*?\d+.*?[\)\]]", "", title)
    title = re.sub(r"ì œ\s*\d+\s*ê¶Œ", "", title)
    title = re.sub(r"\b\d+\s*ê¶Œ\b", "", title)
    title = re.sub(r"vol\.?\s*\d+", "", title, flags=re.I)
    title = re.sub(r"volume\s*\d+", "", title, flags=re.I)
    title = re.sub(r"book\s*\d+", "", title, flags=re.I)
    title = re.sub(r"\s\d{1,2}$", "", title)
    title = re.sub(r"\d{1,2}$", "", title)
    return title.strip()

# ---------------- êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ìˆ˜ì§‘ (ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ 10ê°œ ì œí•œ) ----------------
async def fetch_page(session, page_no, sem, retries=3):
    params = {
        "cert_key": API_KEY, "result_style": "json",
        "page_no": page_no, "page_size": 100,
        "sort": "INPUT_DATE", "order_by": "DESC"
    }
    for attempt in range(1, retries+1):
        try:
            async with sem:  # ë™ì‹œ ìš”ì²­ ì œí•œ
                async with session.get("https://www.nl.go.kr/seoji/SearchApi.do", params=params, headers=headers, timeout=30) as res:
                    res.raise_for_status()
                    return await res.json()
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ (page {page_no}, ì‹œë„ {attempt}): {e}")
            await asyncio.sleep(3)
    return None

async def collect_books_async(start_page, end_page):
    collected_file = f"{DATA_DIR}/books_collected.csv"
    existing_df = pd.read_csv(collected_file) if os.path.exists(collected_file) else pd.DataFrame(columns=["title","author","isbn"])
    collected_isbns = set(existing_df["isbn"])
    title_prefix_set = set(existing_df["title"].apply(clean_title_prefix)) if not existing_df.empty else set()

    print(f"â–¶ êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ë°ì´í„° ìˆ˜ì§‘: {start_page} ~ {end_page} í˜ì´ì§€ (ë™ì‹œ 10ê°œ ì œí•œ)")

    new_books = []
    sem = asyncio.Semaphore(10)  # ë™ì‹œ ìš”ì²­ 10ê°œë¡œ ì œí•œ
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_page(session, p, sem) for p in range(start_page, end_page+1)]
        results = await asyncio.gather(*tasks)

    for data in results:
        if not data:
            continue
        print(f"ğŸ“„ page ì‘ë‹µ: {len(data.get('docs', []))}ê¶Œ")

        for item in data.get("docs", []):
            raw_title = item.get("TITLE", "")
            title = str(raw_title[0]).strip() if isinstance(raw_title, list) else str(raw_title).strip()
            isbn = str(item.get("EA_ISBN", "")).strip()
            author = str(item.get("AUTHOR", "")).strip()
            if not isbn or not title or isbn in collected_isbns:
                continue

            if is_series_volume(title):
                prefix = clean_title_prefix(title)
                if prefix in title_prefix_set:
                    continue
                title_prefix_set.add(prefix)
            else:
                prefix = title.strip()
                if prefix in title_prefix_set:
                    continue
                title_prefix_set.add(prefix)

            new_books.append({"title": title, "author": author, "isbn": isbn})
            collected_isbns.add(isbn)

    final_df = pd.concat([existing_df, pd.DataFrame(new_books)]).drop_duplicates(subset=["isbn"])
    final_df.to_csv(collected_file, index=False, encoding="utf-8-sig")
    print(f"ğŸ“ ëˆ„ì  ë„ì„œ ë°ì´í„°: {len(final_df)}ê¶Œ (ì‹ ê·œ {len(new_books)}ê¶Œ)")

    # ë¸”ë¡ë³„ ì¤‘ê°„ ì €ì¥
    block_file = f"{DATA_DIR}/books_partial_{end_page}.csv"
    pd.DataFrame(new_books).to_csv(block_file, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: {block_file}")

    return pd.DataFrame(new_books)

def collect_books_safe():
    last_page_file = f"{DATA_DIR}/last_page.txt"
    if os.path.exists(last_page_file):
        with open(last_page_file, "r") as f:
            last_page = int(f.read().strip())
    else:
        last_page = 4600  # ì—†ìœ¼ë©´ 4600ì—ì„œ ì‹œì‘
    start_page = last_page + 1
    end_page = start_page + 99

    new_books = asyncio.run(collect_books_async(start_page, end_page))

    # ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•œ last_page ê°±ì‹ 
    with open(last_page_file, "w") as f:
        f.write(str(end_page))

    return new_books

# ---------------- Google Books API (ì¬ì‹œë„ + ë¶€ë¶„ ì €ì¥) ----------------
def fetch_google_book_info(isbn):
    url = f"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}"
    try:
        res = requests.get(url, timeout=10)
        res.raise_for_status()
        data = res.json()
        if "items" not in data:
            return None
        info = data["items"][0]["volumeInfo"]
        return {
            "title": info.get("title", "").strip(),
            "author": ", ".join(info.get("authors", [])),
            "isbn": isbn,
            "description": info.get("description", "").strip()
        }
    except:
        return None

def match_google_books_safe(new_books_df, max_workers=10, chunk_size=1000):
    desc_file = f"{DATA_DIR}/books_with_descriptions.csv"
    existing_desc = pd.read_csv(desc_file) if os.path.exists(desc_file) else pd.DataFrame(columns=["title","author","isbn","description"])
    existing_isbns = set(existing_desc["isbn"])

    target_isbns = [isbn for isbn in new_books_df["isbn"] if isbn not in existing_isbns]
    print(f"â–¶ Google Books ë§¤ì¹­: ëŒ€ìƒ {len(target_isbns)}ê¶Œ")

    matched_books, failed_isbns = [], []

    def fetch_with_retry(isbn, retries=3):
        for attempt in range(1, retries+1):
            result = fetch_google_book_info(isbn)
            if result and result["description"]:
                return result
            time.sleep(1)
        failed_isbns.append(isbn)
        return None

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(fetch_with_retry, isbn): isbn for isbn in target_isbns}
        for i, future in enumerate(as_completed(futures), start=1):
            result = future.result()
            if result:
                matched_books.append(result)
                print(f"âœ… [{i}/{len(target_isbns)}] {result['title']}")
            else:
                print(f"âŒ [{i}/{len(target_isbns)}] ì‹¤íŒ¨ ë˜ëŠ” ì„¤ëª… ì—†ìŒ")

            if i % chunk_size == 0:
                temp_file = f"{DATA_DIR}/descriptions_partial_{i}.csv"
                pd.DataFrame(matched_books).to_csv(temp_file, index=False, encoding="utf-8-sig")
                print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: {temp_file}")

    # ìµœì¢… ë³‘í•©
    new_desc_df = pd.DataFrame(matched_books)
    all_desc_df = pd.concat([existing_desc, new_desc_df]).drop_duplicates(subset=["isbn"])
    all_desc_df.to_csv(desc_file, index=False, encoding="utf-8-sig")

    if failed_isbns:
        log_file = f"{DATA_DIR}/failed_isbns_partial_{len(failed_isbns)}.txt"
        with open(log_file, "w", encoding="utf-8") as f:
            f.write("\n".join(failed_isbns))
        print(f"âš ï¸ ì‹¤íŒ¨ ISBN {len(failed_isbns)}ê°œ ê¸°ë¡: {log_file}")

    print(f"ğŸ“ ì„¤ëª… ë°ì´í„° ê°±ì‹ : ì´ {len(all_desc_df)}ê¶Œ")
    return all_desc_df

# ---------------- FAISS ì ì§„ì  ì—…ë°ì´íŠ¸ ----------------
"""
def update_faiss_index(df_new_desc):
    model = SentenceTransformer("jhgan/ko-sroberta-multitask")
    df_new_desc = df_new_desc.fillna("")
    texts = (df_new_desc["title"] + " " + df_new_desc["author"] + " " + df_new_desc["description"]).astype(str).tolist()
    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)

    index_file = f"{DATA_DIR}/faiss_books.index"
    if os.path.exists(index_file):
        index = faiss.read_index(index_file)
        index.add(embeddings)
    else:
        dim = embeddings.shape[1]
        index = faiss.IndexFlatL2(dim)
        index.add(embeddings)

    faiss.write_index(index, index_file)
    print(f"ğŸ“¦ FAISS ì¸ë±ìŠ¤ ê°±ì‹  ì™„ë£Œ â†’ {index_file}")
"""
# ---------------- ì‹¤í–‰ ----------------
if __name__ == "__main__":
    new_books = collect_books_safe()                     # 4601í˜ì´ì§€(37ë§Œ ì´í›„) ìˆ˜ì§‘
    new_desc = match_google_books_safe(new_books)        # description ìˆ˜ì§‘ (ì•ˆì „ ëª¨ë“œ)
    #update_faiss_index(new_desc)                         # ì‹ ê·œ ë°ì´í„°ë§Œ ì¸ë±ìŠ¤ì— ì¶”ê°€ (ì›í•˜ë©´ ì£¼ì„ ì²˜ë¦¬)
