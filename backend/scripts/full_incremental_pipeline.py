import os
import re
import time
import requests
import pandas as pd
from glob import glob
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

# -------------------------
# ì„¤ì •
# -------------------------
API_KEY = "acb95e5a2989c1fe3507d7119fb16cf35f331355485bf12f2683eb153ccc1f5e"
headers = {"User-Agent": "Mozilla/5.0"}
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

# -------------------------
# 1) êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ 10ë§Œ ê¶Œ ë‹¨ìœ„ ëˆ„ì  ìˆ˜ì§‘
# -------------------------
def is_series_volume(title):
    title = title.lower()
    patterns = [
        r"\b\d+\s*ê¶Œ\b", r"ì œ\s*\d+\s*ê¶Œ", r"\bvol\.?\s*\d+",
        r"\bvolume\s*\d+", r"\bbook\s*\d+", r"\bpart\s*\d+",
        r"ì‹œì¦Œ\s*\d+", r"[\[\(]?\d+\s*ê¶Œ[\]\)]?", r"\s\d{1,2}$", r"[^\d]\d{1,2}$"
    ]
    return any(re.search(p, title) for p in patterns)

def clean_title_prefix(title):
    title = re.sub(r"[\s\(\[].*?\d+.*?[\)\]]", "", title)
    title = re.sub(r"ì œ\s*\d+\s*ê¶Œ", "", title)
    title = re.sub(r"\b\d+\s*ê¶Œ\b", "", title)
    title = re.sub(r"vol\.?\s*\d+", "", title, flags=re.I)
    title = re.sub(r"volume\s*\d+", "", title, flags=re.I)
    title = re.sub(r"book\s*\d+", "", title, flags=re.I)
    title = re.sub(r"\s\d{1,2}$", "", title)
    title = re.sub(r"\d{1,2}$", "", title)
    return title.strip()

def fetch_page(page_no):
    params = {
        "cert_key": API_KEY, "result_style": "json",
        "page_no": page_no, "page_size": 100,
        "sort": "INPUT_DATE", "order_by": "DESC"
    }
    try:
        res = requests.get("https://www.nl.go.kr/seoji/SearchApi.do", params=params, headers=headers, timeout=30)
        res.raise_for_status()
        return res.json()
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ (page {page_no}): {e}")
        return None

def collect_books_incremental():
    collected_file = f"{DATA_DIR}/books_collected.csv"
    existing_df = pd.read_csv(collected_file) if os.path.exists(collected_file) else pd.DataFrame(columns=["title", "author", "isbn"])
    collected_isbns = set(existing_df["isbn"])
    title_prefix_set = set(existing_df["title"].apply(clean_title_prefix)) if not existing_df.empty else set()

      # --- ìˆ˜ì •ëœ ë¶€ë¶„: last_page.txtë¡œ ë§ˆì§€ë§‰ í˜ì´ì§€ ì¶”ì  ---
    last_page_file = f"{DATA_DIR}/last_page.txt"
    if os.path.exists(last_page_file):
        with open(last_page_file, "r") as f:
            last_page = int(f.read().strip())
    else:
        last_page = 0  # ì²« ì‹¤í–‰ ì‹œ 1í˜ì´ì§€ë¶€í„° ì‹œì‘

    start_page = last_page + 1
    end_page = start_page + 999
    print(f"â–¶ êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ë°ì´í„° ìˆ˜ì§‘: {start_page} ~ {end_page} í˜ì´ì§€ (ì•½ 10ë§Œ ê¶Œ)")

    new_books = []
    for page_no in range(start_page, end_page + 1):
        data = fetch_page(page_no)
        if not data:
            continue

        for item in data.get("docs", []):
            raw_title = item.get("TITLE", "")
            title = str(raw_title[0]).strip() if isinstance(raw_title, list) else str(raw_title).strip()
            isbn = str(item.get("EA_ISBN", "")).strip()
            author = str(item.get("AUTHOR", "")).strip()
            if not isbn or not title or isbn in collected_isbns:
                continue

            if is_series_volume(title):
                prefix = clean_title_prefix(title)
                if prefix in title_prefix_set:
                    continue
                title_prefix_set.add(prefix)
            else:
                prefix = title.strip()
                if prefix in title_prefix_set:
                    continue
                title_prefix_set.add(prefix)

            new_books.append({"title": title, "author": author, "isbn": isbn})
            collected_isbns.add(isbn)

        print(f"âœ… í˜ì´ì§€ {page_no} ì™„ë£Œ / ìƒˆë¡œ ìˆ˜ì§‘ {len(new_books)}ê¶Œ")

        # 50í˜ì´ì§€ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
        if page_no % 50 == 0:
            temp_df = pd.concat([existing_df, pd.DataFrame(new_books)]).drop_duplicates(subset=["isbn"])
            temp_df.to_csv(f"{DATA_DIR}/books_partial_{page_no}.csv", index=False, encoding="utf-8-sig")
            print(f"ğŸ’¾ ì¤‘ê°„ ë°±ì—…: {DATA_DIR}/books_partial_{page_no}.csv")

        time.sleep(2)

    # ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
    final_df = pd.concat([existing_df, pd.DataFrame(new_books)]).drop_duplicates(subset=["isbn"])
    final_df.to_csv(collected_file, index=False, encoding="utf-8-sig")
    print(f"ğŸ“ ëˆ„ì  ë„ì„œ ë°ì´í„°: {len(final_df)}ê¶Œ â†’ {collected_file}")

    # ë§ˆì§€ë§‰ í˜ì´ì§€ ê¸°ë¡ (ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì´ì–´ì„œ)
    with open(last_page_file, "w") as f:
        f.write(str(end_page))

    return pd.DataFrame(new_books)  # ì´ë²ˆ ì‹¤í–‰ì˜ ì‹ ê·œ ë„ì„œë§Œ ë°˜í™˜

# -------------------------
# 2) Google Books API - ìƒˆë¡œ ìˆ˜ì§‘í•œ ISBNë§Œ ë§¤ì¹­
# -------------------------
def fetch_google_book_info(isbn):
    url = f"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}"
    try:
        res = requests.get(url, timeout=10)
        res.raise_for_status()
        data = res.json()
        if "items" not in data:
            return None
        info = data["items"][0]["volumeInfo"]
        return {
            "title": info.get("title", "").strip(),
            "author": ", ".join(info.get("authors", [])),
            "isbn": isbn,
            "description": info.get("description", "").strip()
        }
    except:
        return None

def match_google_books(new_books_df):
    # ì´ë¯¸ ì„¤ëª…ì´ ìˆëŠ” ISBN ì œì™¸
    desc_file = f"{DATA_DIR}/books_with_descriptions.csv"
    existing_desc = pd.read_csv(desc_file) if os.path.exists(desc_file) else pd.DataFrame(columns=["title","author","isbn","description"])
    existing_isbns = set(existing_desc["isbn"])

    target_isbns = [isbn for isbn in new_books_df["isbn"] if isbn not in existing_isbns]
    print(f"â–¶ Google Books ë§¤ì¹­: ìƒˆë¡œ ë“¤ì–´ì˜¨ ISBN {len(target_isbns)}ê°œ ì²˜ë¦¬")

    matched_books = []
    for i, isbn in enumerate(target_isbns, start=1):
        book = fetch_google_book_info(isbn)
        if book and book["description"]:
            matched_books.append(book)
            print(f"âœ… [{i}/{len(target_isbns)}] ì„¤ëª… ìˆ˜ì§‘: {book['title']}")
        else:
            print(f"âš ï¸ [{i}/{len(target_isbns)}] ì„¤ëª… ì—†ìŒ")
        time.sleep(0.4)

    # ê¸°ì¡´ ì„¤ëª… ë°ì´í„°ì™€ í•©ì¹˜ê¸°
    all_desc_df = pd.concat([existing_desc, pd.DataFrame(matched_books)]).drop_duplicates(subset=["isbn"])
    all_desc_df.to_csv(desc_file, index=False, encoding="utf-8-sig")
    print(f"ğŸ“ ì„¤ëª… ë°ì´í„° ê°±ì‹ : ì´ {len(all_desc_df)}ê¶Œ ì €ì¥")
    return all_desc_df

# -------------------------
# 3) ì„ë² ë”© + FAISS ì¸ë±ìŠ¤ ìƒì„± (ì „ì²´ ì„¤ëª… ë°ì´í„° ê¸°ì¤€)
# -------------------------
def build_faiss_index(df_desc):
    model = SentenceTransformer("intfloat/multilingual-e5-base")
    texts = (df_desc["title"] + " " + df_desc["author"] + " " + df_desc["description"]).tolist()
    texts = [f"passage: {t}" for t in texts]  # ë¬¸ì„œ í”„ë¦¬í”½ìŠ¤
    embeddings = model.encode(texts, normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=True)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    faiss.write_index(index, f"{DATA_DIR}/faiss_books.index")
    df_desc.to_csv(f"{DATA_DIR}/final_books_for_recommendation.csv", index=False, encoding="utf-8-sig")
    print(f"ğŸ“¦ ì„ë² ë”© ë° ì¸ë±ìŠ¤ ì™„ë£Œ â†’ {DATA_DIR}/faiss_books.index")
    return index

# -------------------------
# ì‹¤í–‰
# -------------------------
if __name__ == "__main__":
    new_books = collect_books_incremental()           # ì´ë²ˆ ì‹¤í–‰ì—ì„œ ìƒˆë¡œ ìˆ˜ì§‘ëœ ë„ì„œë§Œ ë°˜í™˜
    all_desc = match_google_books(new_books)          # ìƒˆë¡œ ë“¤ì–´ì˜¨ ë„ì„œë§Œ Google Books ë§¤ì¹­ í›„ ëˆ„ì  ê°±ì‹ 
    build_faiss_index(all_desc)                       # ì„¤ëª… ìˆëŠ” ì „ì²´ ë°ì´í„°ë¡œ ì¸ë±ìŠ¤ ì¬ìƒì„±
