import os, re, time, requests, asyncio, aiohttp, pandas as pd
from typing import List

# =========================
# ì„¤ì •
# =========================
API_KEY = "acb95e5a2989c1fe3507d7119fb16cf35f331355485bf12f2683eb153ccc1f5e"
headers = {"User-Agent": "Mozilla/5.0"}
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

# ê¸°ì¡´ CSVì˜ ISBNê¹Œì§€ ì •ê·œí™”í•´ ì¬ì €ì¥í• ì§€ ì—¬ë¶€
NORMALIZE_EXISTING = True

# Google Books í˜¸ì¶œ ë™ì‹œì„±(ë„ˆë¬´ í¬ë©´ 429 ìœ„í—˜)
MAX_CONCURRENCY = 5

# =========================
# ISBN ì •ê·œí™” & ê²€ì¦
# =========================
ISBN_CANDIDATE_RE = re.compile(r'(\d{13}|\d{9}[\dXx])')

def _clean_isbn_chars(s: str) -> str:
    if not s:
        return ""
    # í•˜ì´í”ˆ/ìŠ¬ë˜ì‹œ/íŒŒì´í”„ ë“± êµ¬ë¶„ìëŠ” ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ í›„ í† í°í™”
    s = s.replace("-", " ").replace("|", " ").replace("/", " ")
    return s

def _isbn10_to_13(isbn10: str) -> str:
    """í•„ìš” ì‹œ ISBN-10ì„ ISBN-13ìœ¼ë¡œ ë³€í™˜(ì ‘ë‘ 978)"""
    core = "978" + isbn10[:-1]
    total = 0
    for i, ch in enumerate(core):
        d = int(ch)
        total += d if i % 2 == 0 else 3 * d
    check = (10 - (total % 10)) % 10
    return core + str(check)

def is_valid_isbn13(s: str) -> bool:
    if not s or len(s) != 13 or not s.isdigit():
        return False
    total = 0
    for i, ch in enumerate(s):
        d = int(ch)
        total += d if i % 2 == 0 else 3 * d
    return total % 10 == 0

def is_valid_isbn10(s: str) -> bool:
    if not s or len(s) != 10:
        return False
    total = 0
    for i, ch in enumerate(s[:9]):
        if not ch.isdigit():
            return False
        total += (10 - i) * int(ch)
    check = s[9].upper()
    total += 10 if check == "X" else (0 if not check.isdigit() else int(check))
    return total % 11 == 0

def normalize_isbn(raw: str) -> str:
    """ì—¬ëŸ¬ ê°œ ì„ì¸ EA_ISBNì—ì„œ 13ìë¦¬ ìš°ì„ , ì—†ìœ¼ë©´ 10ìë¦¬ ì¤‘ ìœ íš¨í•œ ê²ƒì„ ì„ íƒí•˜ì—¬ 13ìë¦¬ë¡œ í†µì¼"""
    if not raw:
        return ""
    raw = _clean_isbn_chars(str(raw))
    cands = [c.upper() for c in ISBN_CANDIDATE_RE.findall(raw)]
    # 13ìë¦¬ ì¤‘ ìœ íš¨í•œ ê²ƒ ìš°ì„ 
    for c in cands:
        if len(c) == 13 and is_valid_isbn13(c):
            return c
    # 10ìë¦¬ ì¤‘ ìœ íš¨í•œ ê²ƒ â†’ 13ìë¦¬ë¡œ ë³€í™˜
    for c in cands:
        if len(c) == 10 and is_valid_isbn10(c):
            return _isbn10_to_13(c)
    # ê·¸ë˜ë„ ëª» ì°¾ìœ¼ë©´ ìˆ«ìë§Œ ì¶”ë¦° 13ìë¦¬ í›„ë³´(ê²€ì¦ ì—†ì´) ë§ˆì§€ë§‰ ì‹œë„
    for c in cands:
        if len(c) == 13 and c.isdigit():
            return c
    return ""

# =========================
# ì‹œë¦¬ì¦ˆë¬¼ íŒë‹¨ ë° ì •ë¦¬(ì œëª© ì¤‘ë³µ ë°©ì§€ìš©)
# =========================
def is_series_volume(title):
    if not isinstance(title, str):
        return False
    title = title.lower()
    patterns = [
        r"\b\d+\s*ê¶Œ\b", r"ì œ\s*\d+\s*ê¶Œ", r"\bvol\.?\s*\d+",
        r"\bvolume\s*\d+", r"\bbook\s*\d+", r"\bpart\s*\d+",
        r"ì‹œì¦Œ\s*\d+", r"[\[\(]?\d+\s*ê¶Œ[\]\)]?", r"\s\d{1,2}$", r"[^\d]\d{1,2}$"
    ]
    return any(re.search(p, title) for p in patterns)

def clean_title_prefix(title):
    if not isinstance(title, str):
        title = ""
    title = re.sub(r"[\s\(\[].*?\d+.*?[\)\]]", "", title)
    title = re.sub(r"ì œ\s*\d+\s*ê¶Œ", "", title)
    title = re.sub(r"\b\d+\s*ê¶Œ\b", "", title)
    title = re.sub(r"vol\.?\s*\d+", "", title, flags=re.I)
    title = re.sub(r"volume\s*\d+", "", title, flags=re.I)
    title = re.sub(r"book\s*\d+", "", title, flags=re.I)
    title = re.sub(r"\s\d{1,2}$", "", title)
    title = re.sub(r"\d{1,2}$", "", title)
    return title.strip()

# =========================
# êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ API
# =========================
def fetch_page(page_no):
    params = {
        "cert_key": API_KEY,
        "result_style": "json",
        "page_no": page_no,
        "page_size": 100,
        "sort": "INPUT_DATE",
        "order_by": "DESC"
    }
    try:
        res = requests.get("https://www.nl.go.kr/seoji/SearchApi.do", params=params, headers=headers, timeout=30)
        res.raise_for_status()
        return res.json()
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ (page {page_no}): {e}")
        return None

# =========================
# ìˆ˜ì§‘(ë™ê¸°) - ISBN ì •ê·œí™” ì ìš©
# =========================
def collect_books_sync():
    collected_file = f"{DATA_DIR}/books_collected.csv"
    if os.path.exists(collected_file):
        existing_df = pd.read_csv(collected_file)
    else:
        existing_df = pd.DataFrame(columns=["title", "author", "isbn"])

    # (ì˜µì…˜) ê¸°ì¡´ CSVì˜ ISBNë„ ì •ê·œí™”í•´ í†µì¼
    if NORMALIZE_EXISTING and not existing_df.empty:
        existing_df["isbn"] = existing_df["isbn"].astype(str).fillna("").apply(normalize_isbn)
        existing_df = existing_df[existing_df["isbn"] != ""]

    existing_df["title"] = existing_df["title"].fillna("")
    collected_isbns = set(existing_df["isbn"])
    title_prefix_set = set(existing_df["title"].apply(clean_title_prefix)) if not existing_df.empty else set()

    # ì´ì–´ë°›ê¸°
    last_page_file = f"{DATA_DIR}/last_page.txt"
    last_page = int(open(last_page_file).read()) if os.path.exists(last_page_file) else 0
    start_page = last_page + 1
    end_page = start_page + 999

    print(f"\nğŸ“˜ êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ìˆ˜ì§‘ ì‹œì‘: {start_page} ~ {end_page} í˜ì´ì§€ (ë™ê¸° ë°©ì‹)")
    new_books = []

    for page_no in range(start_page, end_page + 1):
        data = fetch_page(page_no)
        if not data or "docs" not in data:
            continue

        for item in data["docs"]:
            raw_title = str(item.get("TITLE", "")).strip()
            raw_isbn = str(item.get("EA_ISBN", "")).strip()
            author = str(item.get("AUTHOR", "")).strip()

            isbn = normalize_isbn(raw_isbn)
            if not isbn or not raw_title or isbn in collected_isbns:
                continue

            if is_series_volume(raw_title):
                prefix = clean_title_prefix(raw_title)
                if prefix in title_prefix_set:
                    continue
                title_prefix_set.add(prefix)
            else:
                prefix = raw_title.strip()
                if prefix in title_prefix_set:
                    continue
                title_prefix_set.add(prefix)

            new_books.append({"title": raw_title, "author": author, "isbn": isbn})
            collected_isbns.add(isbn)

        if page_no % 50 == 0:
            print(f"ğŸ“¥ {page_no} í˜ì´ì§€ ì™„ë£Œ - ëˆ„ì  ìˆ˜ì§‘ {len(new_books)}ê¶Œ")
        time.sleep(1.2)

    # ì €ì¥(ì •ê·œí™”ëœ ISBN ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°)
    final_df = pd.concat([existing_df, pd.DataFrame(new_books)], ignore_index=True)
    final_df = final_df.drop_duplicates(subset=["isbn"])
    final_df.to_csv(collected_file, index=False, encoding="utf-8-sig")

    with open(last_page_file, "w") as f:
        f.write(str(end_page))

    print(f"\nğŸ“ ëˆ„ì  ë„ì„œ ë°ì´í„°: ì´ {len(final_df)}ê¶Œ (ì‹ ê·œ {len(new_books)}ê¶Œ)")
    return pd.DataFrame(new_books)

# =========================
# Google Books API ì„¤ëª… ìˆ˜ì§‘ (ë¹„ë™ê¸° + ì§€ìˆ˜ ë°±ì˜¤í”„)
# =========================
async def fetch_google_description(session, isbn: str, retries=3):
    url = f"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}"
    backoff = 0.5
    for attempt in range(1, retries + 1):
        try:
            await asyncio.sleep(backoff)  # 0.5 â†’ 1.0 â†’ 2.0 ...
            async with session.get(url, timeout=15) as res:
                if res.status == 429:
                    # Too Many Requests â†’ ë°±ì˜¤í”„ í›„ ì¬ì‹œë„
                    raise aiohttp.ClientResponseError(res.request_info, res.history, status=429, message="Too Many Requests")
                res.raise_for_status()
                data = await res.json()
                if "items" not in data or not data["items"]:
                    raise ValueError("No items in response")
                info = data["items"][0].get("volumeInfo", {})
                description = (info.get("description") or "").strip()
                if not description:
                    raise ValueError("No description found")
                return {
                    "title": (info.get("title") or "").strip(),
                    "author": ", ".join(info.get("authors", [])) if info.get("authors") else "",
                    "isbn": isbn,  # ì •ê·œí™”ëœ ê°’
                    "description": description
                }
        except Exception as e:
            print(f"âš ï¸ [{isbn}] ì‹œë„ {attempt} ì‹¤íŒ¨: {e}")
            backoff *= 2
    return None

async def match_google_books(isbn_list: List[str]):
    desc_file = f"{DATA_DIR}/books_with_descriptions.csv"
    if os.path.exists(desc_file):
        existing_df = pd.read_csv(desc_file)
    else:
        existing_df = pd.DataFrame(columns=["title", "author", "isbn", "description"])

    # (ì˜µì…˜) ê¸°ì¡´ ì„¤ëª… CSVë„ ì •ê·œí™”í•´ í†µì¼
    if NORMALIZE_EXISTING and not existing_df.empty:
        existing_df["isbn"] = existing_df["isbn"].astype(str).fillna("").apply(normalize_isbn)
        existing_df = existing_df[existing_df["isbn"] != ""]

    # íƒ€ê¹ƒë„ ì •ê·œí™”
    norm_targets = []
    for x in isbn_list:
        nx = normalize_isbn(x)
        if nx:
            norm_targets.append(nx)

    existing_isbns = set(existing_df["isbn"])
    targets = [t for t in norm_targets if t not in existing_isbns]
    print(f"\nğŸ“— Google ì„¤ëª… ìˆ˜ì§‘ ëŒ€ìƒ: {len(targets)}ê¶Œ")

    matched_books = []
    sem = asyncio.Semaphore(MAX_CONCURRENCY)

    async def worker(isbn):
        async with sem:
            book = await fetch_google_description(session, isbn)
            if book:
                matched_books.append(book)
                print(f"âœ… {book['title']} ({isbn})")
            else:
                print(f"âŒ {isbn} ìµœì¢… ì‹¤íŒ¨")

    async with aiohttp.ClientSession() as session:
        # ìˆœì°¨ ëŒ€ì‹  íƒœìŠ¤í¬ ë³‘ë ¬
        tasks = [asyncio.create_task(worker(isbn)) for isbn in targets]
        await asyncio.gather(*tasks)

    updated_df = pd.concat([existing_df, pd.DataFrame(matched_books)], ignore_index=True)
    updated_df = updated_df.drop_duplicates(subset=["isbn"])
    updated_df.to_csv(desc_file, index=False, encoding="utf-8-sig")

    # ì‹¤íŒ¨ ëª©ë¡ íŒŒì¼
    failures = [t for t in targets if t not in set(updated_df["isbn"])]
    if failures:
        fail_file = f"{DATA_DIR}/failed_google_books.txt"
        with open(fail_file, "w") as f:
            f.write("\n".join(failures))
        print(f"âš ï¸ ì‹¤íŒ¨í•œ ISBN {len(failures)}ê¶Œ â†’ {fail_file} ì €ì¥ë¨")

    print(f"\nğŸ“ ì„¤ëª… í¬í•¨ ë„ì„œ ì´: {len(updated_df)}ê¶Œ")
    return updated_df

# =========================
# ì‹¤í–‰ë¶€
# =========================
if __name__ == "__main__":
    new_books = collect_books_sync()
    if not new_books.empty:
        asyncio.run(match_google_books(new_books["isbn"].tolist()))
    else:
        print("ğŸ›‘ ì‹ ê·œ ë„ì„œ ì—†ìŒ, ì„¤ëª… ìˆ˜ì§‘ ìƒëµ")
