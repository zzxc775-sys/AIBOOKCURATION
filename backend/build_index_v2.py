"""
build_index_v2.py (ë©”ëª¨ë¦¬ ì•ˆì „ ë²„ì „)
- ì„ë² ë”©ì„ í•œ ë²ˆì— ì „ë¶€ ëª¨ìœ¼ì§€ ì•Šê³ , ë°°ì¹˜ë³„ë¡œ FAISSì— ë°”ë¡œ add() í•˜ëŠ” ë°©ì‹(ìŠ¤íŠ¸ë¦¬ë°)
- meta.parquetëŠ” row_id + (title, author, description)ë§Œ ì €ì¥

ì‹¤í–‰:
  (PowerShell)
  $env:EMBED_BATCH_SIZE="32"
  $env:MAX_CHARS="1200"
  $env:MAX_SEQ_LEN="256"
  python backend/build_index_v2.py
"""

from __future__ import annotations

import os
from pathlib import Path
from typing import Tuple

import numpy as np
import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer


# -------------------------
# ê²½ë¡œ/ì„¤ì •
# -------------------------
BASE_DIR = Path(__file__).resolve().parent  # backend/
DEFAULT_INPUT = BASE_DIR / "data" / "books_with_descriptions"
OUT_DIR = BASE_DIR / "models" / "faiss_index_v2"

MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "intfloat/multilingual-e5-base")
DEVICE = os.getenv("EMBED_DEVICE", "cpu")

# âœ… ê¸°ë³¸ê°’ì„ ì•ˆì „í•˜ê²Œ ë‚®ì¶¤ (ê¸°ì¡´ 2048ì€ CPUì—ì„œ í„°ì§ˆ í™•ë¥  ë†’ìŒ)
BATCH_SIZE = int(os.getenv("EMBED_BATCH_SIZE", "32"))

# âœ… í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° í­ë°œ ë°©ì§€)
MAX_CHARS = int(os.getenv("MAX_CHARS", "1200"))

# âœ… ëª¨ë¸ max_seq_length ì œí•œ (í† í° í­ë°œ ë°©ì§€)
MAX_SEQ_LEN = int(os.getenv("MAX_SEQ_LEN", "256"))


def _resolve_input_path(p: Path) -> Path:
    if p.exists() and p.is_file():
        return p

    candidates = [p.with_suffix(".csv"), p.with_suffix(".parquet"), p.with_suffix(".pq")]
    for c in candidates:
        if c.exists() and c.is_file():
            return c

    if p.exists() and p.is_dir():
        for ext in ("*.parquet", "*.pq", "*.csv"):
            files = sorted(p.glob(ext))
            if files:
                return files[0]

    raise FileNotFoundError(f"ì…ë ¥ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {p} (ë˜ëŠ” {candidates})")


def _load_dataframe(input_path: Path) -> pd.DataFrame:
    input_path = _resolve_input_path(input_path)

    if input_path.suffix.lower() in (".parquet", ".pq"):
        return pd.read_parquet(input_path)
    if input_path.suffix.lower() == ".csv":
        return pd.read_csv(input_path)

    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ì: {input_path.suffix}")


def _ensure_columns(df: pd.DataFrame) -> pd.DataFrame:
    required = ["title", "author", "description"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing} / í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")

    df = df.copy()
    for c in required:
        df[c] = df[c].fillna("").astype(str)

    df = df.reset_index(drop=True)
    df["row_id"] = df.index.astype(np.int64)
    return df[["row_id", "title", "author", "description"]]


def _make_passage_text(title: str, author: str, desc: str) -> str:
    # âœ… ë„ˆë¬´ ê¸´ ì„¤ëª…ì€ ì˜ë¼ì„œ í† í° í­ë°œ ë°©ì§€
    desc = (desc or "")[:MAX_CHARS]

    # E5 passage prefix
    return (
        "passage: "
        f"ì œëª©: {title} / ì €ì: {author}\n"
        f"ì„¤ëª…: {desc}"
    )


def build_index_v2(input_path: Path = DEFAULT_INPUT, out_dir: Path = OUT_DIR) -> Tuple[Path, Path]:
    out_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ“¥ ë°ì´í„° ë¡œë“œ: {input_path}")
    df = _load_dataframe(input_path)
    print(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(df):,} rows")

    df_meta = _ensure_columns(df)
    print(f"âœ… ë©”íƒ€ ì •ë¦¬ ì™„ë£Œ (ì €ì¥ ì»¬ëŸ¼: {list(df_meta.columns)})")

    print(f"ğŸ¤– ëª¨ë¸ ë¡œë“œ: {MODEL_NAME} / device={DEVICE}")
    model = SentenceTransformer(MODEL_NAME, device=DEVICE)

    # âœ… ëª¨ë¸ ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ(ê°€ëŠ¥í•˜ë©´)
    try:
        model.max_seq_length = MAX_SEQ_LEN
        print(f"âœ… model.max_seq_length = {MAX_SEQ_LEN}")
    except Exception:
        print("âš ï¸ model.max_seq_length ì„¤ì • ì‹¤íŒ¨(ëª¨ë¸/ë²„ì „ ì°¨ì´ì¼ ìˆ˜ ìˆìŒ). ê·¸ë˜ë„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

    # 1) ì²« ë°°ì¹˜ë¡œ dim ì•Œì•„ë‚´ê³  FAISS index ìƒì„±
    n = len(df_meta)
    if n == 0:
        raise ValueError("ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    # ì²« ë°°ì¹˜ í…ìŠ¤íŠ¸ ì¤€ë¹„
    first_end = min(BATCH_SIZE, n)
    first_texts = [
        _make_passage_text(
            df_meta.loc[i, "title"],
            df_meta.loc[i, "author"],
            df_meta.loc[i, "description"],
        )
        for i in range(0, first_end)
    ]

    print("ğŸ§  ì„ë² ë”© ì‹œì‘â€¦ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹: ë°°ì¹˜ë³„ë¡œ FAISSì— ë°”ë¡œ add)")
    first_emb = model.encode(
        first_texts,
        batch_size=min(BATCH_SIZE, len(first_texts)),
        normalize_embeddings=True,
        convert_to_numpy=True,
        show_progress_bar=True,
    ).astype(np.float32)

    dim = int(first_emb.shape[1])
    index = faiss.IndexFlatL2(dim)
    index.add(first_emb)
    print(f"âœ… ì²« ë°°ì¹˜ add ì™„ë£Œ: {index.ntotal:,}/{n:,} (dim={dim})")

    # 2) ë‚˜ë¨¸ì§€ ë°°ì¹˜ ë°˜ë³µ (ì„ë² ë”©ì„ RAMì— ìŒ“ì§€ ì•ŠìŒ)
    for start in range(first_end, n, BATCH_SIZE):
        end = min(start + BATCH_SIZE, n)

        texts = [
            _make_passage_text(
                df_meta.loc[i, "title"],
                df_meta.loc[i, "author"],
                df_meta.loc[i, "description"],
            )
            for i in range(start, end)
        ]

        emb = model.encode(
            texts,
            batch_size=min(BATCH_SIZE, len(texts)),
            normalize_embeddings=True,
            convert_to_numpy=True,
            show_progress_bar=False,
        ).astype(np.float32)

        index.add(emb)

        if (end % (BATCH_SIZE * 20) == 0) or (end == n):
            print(f"ğŸ§± ì§„í–‰: {end:,}/{n:,} | index.ntotal={index.ntotal:,}")

    print(f"ğŸ‰ ì„ë² ë”©/ì¸ë±ì‹± ì™„ë£Œ: ntotal={index.ntotal:,}")

    # 3) ì €ì¥
    index_path = out_dir / "index.faiss"
    meta_path = out_dir / "meta.parquet"

    faiss.write_index(index, str(index_path))
    df_meta.to_parquet(meta_path, index=False)

    print("ğŸ’¾ ì €ì¥ ì™„ë£Œ")
    print(f"- {index_path} ({index_path.stat().st_size / (1024*1024):.2f} MB)")
    print(f"- {meta_path} ({meta_path.stat().st_size / (1024*1024):.2f} MB)")
    return index_path, meta_path


if __name__ == "__main__":
    input_env = os.getenv("INPUT_PATH")
    in_path = Path(input_env) if input_env else DEFAULT_INPUT
    build_index_v2(in_path, OUT_DIR)
