# core/retriever.py
# FAISS ì¸ë±ìŠ¤ êµ¬ì¶•/ë¡œë“œ + ê²€ìƒ‰ (E5 ëª¨ë¸ ìµœì í™”, ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬)

from __future__ import annotations
import os
from typing import List, Dict, Optional
from pathlib import Path

import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document

class BookIndexer:
    """
    CSV -> Documents -> FAISS ì¸ë±ìŠ¤ (í´ë”) ì €ì¥
    - E5 ëª¨ë¸ ê¶Œì¥: ë¬¸ì„œì—ëŠ” 'passage:' ì ‘ë‘ì–´, ì¿¼ë¦¬ì—ëŠ” 'query:' ì ‘ë‘ì–´
    - ëŒ€ìš©ëŸ‰(8ë§Œ+)ì„ ìœ„í•´ ë°°ì¹˜ë¡œ ì¶”ê°€(add_documents) ìˆ˜í–‰
    """
    def __init__(
        self,
        model_name: str = "intfloat/multilingual-e5-base",
        device: str = "cpu",
        normalize: bool = True,
    ):
        self.embedding = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={"device": device},
            encode_kwargs={"normalize_embeddings": normalize},
        )

    @staticmethod
    def _pick_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
        # í›„ë³´ ì»¬ëŸ¼ ì¤‘ ì¡´ì¬í•˜ëŠ” ì²« ë²ˆì§¸ë¥¼ ë°˜í™˜
        for c in candidates:
            if c in df.columns:
                return c
        return None

    def build_index_from_csv(
        self,
        csv_path: str = "data/books_with_descriptions.csv",
        index_dir: str = "models/faiss_index",
        batch_size: int = 2048,
        verbose: bool = True,
    ):
        assert os.path.exists(csv_path), f"CSV not found: {csv_path}"
        os.makedirs(index_dir, exist_ok=True)

        df = pd.read_csv(csv_path)
        if verbose:
            print(f"âœ… CSV ë¡œë“œ: {csv_path} / {len(df):,}í–‰")

        # ì»¬ëŸ¼ ë§¤í•‘(ìœ ì—°í•˜ê²Œ ë‹¤ì–‘í•œ ì´ë¦„ ëŒ€ì‘)
        title_col = self._pick_col(df, ["title", "ë„ì„œëª…"])
        author_col = self._pick_col(df, ["author", "ì €ì"])
        desc_col = self._pick_col(df, ["description", "desc", "summary", "summery", "ì„¤ëª…", "ìš”ì•½"])
        isbn_col = self._pick_col(df, ["isbn", "ISBN", "êµ­ì œí‘œì¤€ë„ì„œë²ˆí˜¸(ISBN)"])
        publisher_col = self._pick_col(df, ["publisher", "ì¶œíŒì‚¬"])

        if title_col is None:
            raise ValueError("ì œëª© ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì˜ˆ: title, ë„ì„œëª…)")
        # author/description/isbn/publisherëŠ” ì—†ì–´ë„ Noneìœ¼ë¡œ ì²˜ë¦¬

        def row_to_doc(row) -> Document:
            title = str(row.get(title_col, "") or "").strip()
            author = str(row.get(author_col, "") or "").strip() if author_col else ""
            desc = str(row.get(desc_col, "") or "").strip() if desc_col else ""
            isbn = str(row.get(isbn_col, "") or "").strip() if isbn_col else ""
            publisher = str(row.get(publisher_col, "") or "").strip() if publisher_col else ""

            text = f"passage: ì œëª©: {title}"
            if author:
                text += f" / ì €ì: {author}"
            if publisher:
                text += f" / ì¶œíŒì‚¬: {publisher}"
            if desc:
                text += f"\nì„¤ëª…: {desc}"

            meta: Dict[str, str] = {
                "title": title,
                "author": author or None,
                "publisher": publisher or None,
                "isbn": isbn or None,
                "description": desc or None,
            }
            return Document(page_content=text, metadata=meta)

        # ë°°ì¹˜ë¡œ ì¸ë±ìŠ¤ ìƒì„±/ì¶”ê°€
        vector_db: Optional[FAISS] = None
        total = len(df)
        for start in range(0, total, batch_size):
            end = min(start + batch_size, total)
            chunk = df.iloc[start:end]
            docs = [row_to_doc(r) for _, r in chunk.iterrows()]

            if vector_db is None:
                vector_db = FAISS.from_documents(docs, self.embedding)
            else:
                vector_db.add_documents(docs)

            if verbose:
                print(f"ğŸ§± ë°°ì¹˜ ì¶”ê°€: {start:,}~{end-1:,} / ëˆ„ì  {end:,}")

        assert vector_db is not None, "ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        vector_db.save_local(index_dir)
        if verbose:
            print(f"ğŸ‰ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_dir}")

class BookRetriever:
    """
    ì €ì¥ëœ FAISS ì¸ë±ìŠ¤(í´ë”)ë¥¼ ë¡œë“œí•´ì„œ ê²€ìƒ‰
    - E5 ëª¨ë¸: ì¿¼ë¦¬ì— 'query:' ì ‘ë‘ì–´ë¥¼ ë¶™ì¸ ë²¡í„°ë¡œ ê²€ìƒ‰
    - ì ìˆ˜ëŠ” Relevance Score(0~1)ë¡œ ë°˜í™˜
    """
    def __init__(
        self,
        index_dir: str = "models/faiss_index",
        model_name: str = "intfloat/multilingual-e5-base",
        device: str = "cpu",
        normalize: bool = True,
    ):
        self.embedding = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={"device": device},
            encode_kwargs={"normalize_embeddings": normalize},
        )
        # â— ë””ë ‰í„°ë¦¬ ë¡œë“œ (pkl í¬í•¨) -> allow_dangerous_deserialization í•„ìš”í•  ìˆ˜ ìˆìŒ
        self.vs: FAISS = FAISS.load_local(
            index_dir,
            self.embedding,
            allow_dangerous_deserialization=True
        )

        # core/retriever.py (íƒìƒ‰ í•¨ìˆ˜ë§Œ êµì²´)
    def retrieve(self, query: str, top_k: int = 5):
        qtext = f"query: {query}"  # E5 ê·œì¹™

        # LangChain FAISS: ê±°ë¦¬ í¬í•¨ ê²€ìƒ‰
        pairs = None
        if hasattr(self.vs, "similarity_search_with_score"):
            pairs = self.vs.similarity_search_with_score(qtext, k=top_k)  # [(doc, dist), ...]
        else:
            # êµ¬ë²„ì „ fallback (ì ìˆ˜ ì—†ì´ ë¬¸ì„œë§Œ)
            docs = self.vs.similarity_search(qtext, k=top_k)
            pairs = [(d, None) for d in docs]

        rows = []
        for doc, dist in pairs:
            # ê±°ë¦¬ â†’ ì½”ì‚¬ì¸(0~1). (ë‹¨ìœ„ë²¡í„° ê°€ì •: squared_L2 = 2 - 2cos â†’ cos = 1 - d/2)
            if dist is None:
                # fallback: ë­í¬ ê¸°ë°˜ ëŒ€ì¶© ê°’ (ìµœí›„ìˆ˜ë‹¨)
                cosine = 1.0
            else:
                cosine = max(0.0, min(1.0, 1.0 - float(dist) / 2.0))
            rows.append((doc, dist, cosine))

        # ìƒëŒ€ ì •ê·œí™”(ì„¸íŠ¸ ë‚´) + ë°”ë‹¥ê°’(10%)ë¡œ 0% ë°©ì§€
        cosines = [c for _, _, c in rows]
        cmax, cmin = (max(cosines) if cosines else 1.0), (min(cosines) if cosines else 0.0)
        eps = 1e-8
        rels = [ (c - cmin) / (cmax - cmin + eps) for c in cosines ]
        rels = [ 0.10 + r * 0.90 for r in rels ]  # 10% ~ 100%

        # ë³„ì : 0.5~5.0, 0.5ë‹¨ìœ„ ë°˜ì˜¬ë¦¼
        def to_stars(c):
            return max(0.5, round(c * 5 * 2) / 2)

        results = []
        for rank, ((doc, dist, cos), rel) in enumerate(zip(rows, rels), start=1):
            meta = dict(doc.metadata or {})
            results.append({
                "title": meta.get("title") or "",
                "author": meta.get("author"),
                "publisher": meta.get("publisher"),
                "isbn": meta.get("isbn"),
                "content": meta.get("description") or "",
                # --- ì—¬ëŸ¬ ì ìˆ˜ ì§€í‘œë¥¼ í•¨ê»˜ ì œê³µ ---
                "rank": rank,                           # 1,2,3â€¦
                "score": round(cos, 3),                 # ì½”ì‚¬ì¸(0~1)
                "score_pct": int(round(cos * 100)),     # 0~100%
                "rel_pct": int(round(rel * 100)),       # 10~100% (ì„¸íŠ¸ ë‚´)
                "stars": to_stars(cos),                 # 0.5~5.0
                "distance": float(dist) if dist is not None else None,  # ì›ì‹œ ê±°ë¦¬(ë””ë²„ê¹…ìš©)
            })

        # ì½”ì‚¬ì¸ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ
        return sorted(results, key=lambda x: x["score"], reverse=True)

